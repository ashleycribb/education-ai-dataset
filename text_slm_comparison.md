| Model Name                       | Parameter Count (Size) | License Type                  | Primary Tasks / Training Focus                                                                    | Hugging Face Fine-tuning Info Available?                                       | Explicit Multimodal Capabilities? |
|----------------------------------|------------------------|-------------------------------|---------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|-----------------------------------|
| `microsoft/Phi-3-mini-4k-instruct` | 3.8B (3.82B actual)    | MIT                           | Instruction-following, chat, strong reasoning (math, logic). Trained on public & synthetic data.  | Yes, sample script (`sample_finetune.py`) for SFT with TRL & Accelerate.     | No (text-only)                    |
| `google/gemma-2b-it`             | 2.51B                  | gemma (custom permissive)     | Instruction-tuned for text generation, QA, summarization, reasoning. Uses chat template.        | Yes, scripts in `google/gemma-7b` repo (adaptable), Colab notebook example.  | No (text-only)                    |
| `Qwen/Qwen1.5-1.8B-Chat`         | 1.84B                  | tongyi-qianwen-research       | Chat model, multilingual. Pretrained + SFT & DPO.                                                 | Yes, via linked GitHub repo and community resources (Adapters/Finetunes).    | No (text-only)                    |
