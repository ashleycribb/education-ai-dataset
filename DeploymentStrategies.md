# AITA Ecosystem: Deployment Strategies

## 1. Introduction

### Purpose
This document outlines deployment considerations for the AI Tutor (AITA) ecosystem developed in this project. It covers both the current local development and testing setup, and provides a conceptual overview of how these components might evolve into a production-ready deployment.

### Current Stage
The strategies discussed for production deployment are **conceptual** at this stage. The current project focuses on building and demonstrating the core components and their interactions in a local environment. A full production deployment would require further engineering, security hardening, and scalability considerations.

## 2. Local Development & Testing Setup

The current prototype system operates effectively in a local environment, allowing for development, testing, and demonstration of core AITA functionalities.

### AITA CLI with Mock LMS
This setup simulates the core interaction between an AITA and an LMS.

*   **Running the Interaction**:
    *   The mock LMS (`lms_mcp_server_mock.py`) and the AITA client (`aita_mcp_client.py`) are run concurrently, with the server's output piped to the client's input:
        ```bash
        python lms_mcp_server_mock.py | python aita_mcp_client.py
        ```
    *   This command uses standard input/output (stdio) for Model Context Protocol (MCP) communication between the two components.

*   **Local Model & Service Loading**:
    *   `aita_mcp_client.py` loads the Small Language Model (SLM) (e.g., Microsoft's Phi-3-mini) and any fine-tuned LoRA adapters directly from the local filesystem (or Hugging Face cache).
    *   The `moderation_service.py` (containing `ModerationService`) is also loaded as a local Python module by `aita_mcp_client.py`, and the moderation model (e.g., `unitary/toxic-bert`) is loaded locally.

*   **Local Data Generation**:
    *   Interaction logs, structured as xAPI-like statements, are generated and saved locally in a file named `xapi_statements.jsonl` by `aita_mcp_client.py`.

### Teacher Dashboard
The `teacher_dashboard_prototype.py` provides a way to visualize these locally generated logs.

*   **Running the Dashboard**:
    *   It's a Streamlit application, started with the command:
        ```bash
        streamlit run teacher_dashboard_prototype.py
        ```
*   **Data Source**:
    *   The dashboard reads directly from the local `xapi_statements.jsonl` file generated by the AITA client. If the file is not found, it uses internal placeholder data for demonstration.

### SDKs (`k12_mcp_server_sdk` & `k12_mcp_client_sdk`)
*   These SDKs are currently used directly from the project structure. Python's import system resolves them as local packages.
*   They are not installed as external libraries in this phase but are integral to the functioning of the mock server and client examples.

## 3. Conceptual Production Deployment Strategy

Transitioning the AITA ecosystem to a production environment would involve evolving the current components into scalable, robust, and secure services.

### AITA Interaction Service (Backend)
This would be the core service handling AITA logic.

*   **Evolution**: The Python logic within `aita_mcp_client.py` (excluding the CLI interaction loop) would be refactored into a web service backend using a framework like FastAPI or Flask.
*   **SLM Hosting**:
    *   **Self-Hosted GPU Server**: Deploy the base SLM and fine-tuned adapters on a dedicated server with GPU(s). This offers control but requires infrastructure management.
    *   **Cloud Inference Endpoints**: Utilize services like Hugging Face Inference Endpoints, AWS SageMaker, Google Vertex AI, or Azure ML to host the SLM. This can simplify scaling and management.
*   **Moderation Service**:
    *   Could be integrated directly into the AITA Interaction Service.
    *   Alternatively, deployed as a separate microservice, especially if used by other components or if it requires distinct scaling.

### Student Frontend (Multi-Device Support)
To ensure accessibility across various devices used in K-12 settings:

*   **Architectural Recommendation**: A responsive **web application** is highly recommended. This can be built using standard HTML, CSS, JavaScript, and a frontend framework (e.g., React, Vue, Angular, Svelte).
*   **Communication**: The student frontend would communicate with the AITA Interaction Service backend via HTTP/HTTPS (e.g., RESTful APIs or WebSockets for more interactive chat).

### MCP Servers (LMS, PassageDB, Ontology - as separate services)
The functionalities mocked by `lms_mcp_server_mock.py` (and other conceptual context providers like PassageDB or an Ontology service) would become independent services.

*   **Deployment**: Each would be deployed as a separate (micro)service.
*   **Communication**: They would expose their resources and tools via MCP over HTTP, using `MCPHTTPServer` (from the `modelcontextprotocol` library) or a similar implementation.
*   **Authentication/Authorization**: Secure communication between these services and the AITA Interaction Service would be crucial, requiring mechanisms like API keys, OAuth2, or other token-based authentication.

### Learning Record Store (LRS)
*   The local `xapi_statements.jsonl` file would be replaced by a proper **Learning Record Store (LRS)**.
*   The AITA Interaction Service would send xAPI statements to this LRS for persistent storage and analysis. Examples include Yet LRS, Learning Locker, or commercial LRS products.

### Teacher Oversight Dashboard
*   The Streamlit-based `teacher_dashboard_prototype.py` would evolve into a more robust web application, potentially part of a larger administrative portal.
*   Its backend would query the LRS for xAPI statements and potentially other services (e.g., user management, course information) to provide comprehensive views for teachers and administrators.

### Databases (PassageDB, Ontology Store)
*   **PassageDB**: Educational content (passages, questions, etc.) would reside in a production-grade database (e.g., PostgreSQL, MySQL, or a NoSQL alternative). The PassageDB MCP server would provide access to this content.
*   **Ontology Store**: If an ontology is used (for tagging content, tracking student knowledge, etc.), it would also need a persistent store, accessible via its own MCP server.

### Containerization
*   **Docker**: Each service (AITA Interaction Service, MCP Servers, Moderation Service, Teacher Dashboard backend) should be containerized using Docker. This simplifies deployment, scaling, and ensures consistency across environments.
*   **Kubernetes**: For orchestrating multiple containerized services in a production environment, Kubernetes (or a managed Kubernetes service from a cloud provider) would be a strong candidate.

## 4. Data Flow in Production

A simplified textual representation of data flow:

1.  **Student Interaction**:
    *   Student Frontend `->` (HTTP request with user input) `->` AITA Interaction Service
2.  **AITA Processing**:
    *   AITA Interaction Service `->` (MCP HTTP request) `->` LMS Context MCP Server (for student/activity data)
    *   AITA Interaction Service `->` (MCP HTTP request) `->` PassageDB MCP Server (for full passage text, etc.)
    *   AITA Interaction Service `->` (Internal call or HTTP request) `->` Moderation Service (for input moderation)
    *   AITA Interaction Service `->` (Call to SLM) `->` SLM Hosting (self-hosted or cloud endpoint)
    *   AITA Interaction Service `->` (Internal call or HTTP request) `->` Moderation Service (for SLM output moderation)
    *   AITA Interaction Service `->` (HTTP response with AITA output) `->` Student Frontend
3.  **Logging**:
    *   AITA Interaction Service `->` (xAPI statement) `->` Learning Record Store (LRS)
4.  **Teacher Dashboard**:
    *   Teacher Dashboard Backend `->` (Queries) `->` Learning Record Store (LRS)
    *   Teacher Dashboard Backend `->` (Potentially queries other services like User Management, Course Info)

## 5. User Profile Management (Future Consideration)

For a production system serving actual students and educators (including homeschool parents), a robust **User Authentication and Profile Management** system is essential. This system would handle:
*   User registration and login for students, teachers, and parents.
*   Role-based access control (RBAC).
*   Storage of user profiles, including learning history, preferences, and potentially anonymized performance data.
*   Integration with the LRS to associate xAPI statements with specific users.
*   Mechanisms for ensuring data privacy and compliance with regulations (e.g., FERPA, GDPR).

This component would likely be a central service interacting with many other parts of the ecosystem.
